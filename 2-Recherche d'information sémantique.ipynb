{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zakDarmoise/tp-information-retrieval-with-llm-student-version/blob/main/2-Recherche%20d'information%20sémantique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2. - Recherche d'Information sémantique\n",
    "\n",
    "Les recherches réalisées dans le TP précédent sont principalement des recherches par mots ou par phrases basés sur le modèle `tf-idf`. Ce dernier construit un espace vectoriel dont la taille est égal au nombre total de tokens distincts dans la collection de documents. L'image ci-dessous représente un espace vectoriel avec 3 tokens distincts. Imaginez ce que cela donnerait avec 100,000 tokens distincts !\n",
    "\n",
    "![tf-df-vector-space](./resources/tfidf-vector-space.png)\n",
    "\n",
    "Certes, des techniques existent pour limiter l'impact des variations syntaxiques (bas/haut de casses, mots au pluriel/singulier, synonymes) mais cela pose plusieurs problèmes :\n",
    "- effort requis pour paramétrer minutieusement la construction des tokens ;\n",
    "- recourt à des dictionnaires, notamment pour les synonymes ;\n",
    "- prise en compte des points précédents pour différentes langues ;\n",
    "- sens d'une phrase, paragraphe, document non pris en compte dans sa globalité.\n",
    "\n",
    "Pour palier ces problèmes, on peut utiliser des techniques avancées de Traitement Automatique du Langage Naturel (TALN) pour construire des espaces vectoriels _sémantiques_ où les mots, paragraphes, documents sont représentés par des vecteurs, appelés _embeddings_, encodant le sens des informations plutôt que leur syntaxe. Les espace vectoriels associés ont une taille fixe, de quelques centaines de dimensions. Ci-dessous un exemple de ce type d'espace en 2 dimensions (source : https://dev.to/jinglescode/word-embeddings-16hb)\n",
    "\n",
    "![embeddings](./resources/embeddings_2d.png)\n",
    "\n",
    "Grâce aux _modèles de langue_, notamment aux _transformers_ (https://arxiv.org/abs/1706.03762) pré-entraînés et proposés librement par des sociétés comme Google, OpenAI, Facebook, il est maintenant possible de construire ses propres _embeddings_ sur n'importe quel texte. \n",
    "\n",
    "Huggingface (https://huggingface.co/) est une plateforme proposant nombre de ces modèles pré-entraînés, en particulier des modèles de type _transformers_ que l'on peut utiliser sur nos propres données : https://huggingface.co/sentence-transformers. \n",
    "\n",
    "Dans ce TP, nous allons utiliser l'un de ces modèles pour construire un _embedding_ par document et nous permettre de faire des recherches sémantiques et de trouver des documents similaires.\n",
    "\n",
    "En sortie de ce module, vous serez capable de :\n",
    "\n",
    "- Calculer l'_embedding_ d'un texte, c'est à dire sa représentation sémantique. En fonction du modèle choisi pour calculer les embeddings, ces derniers peuvent même être multilangues !\n",
    "- Rechercher des documents de manière plus pertinentes grâce à la recherchs sémantique ;\n",
    "- Mettre en oeuvre un système de Question / Réponse en utilisant la méthologie _Retrieval Augmented Generation (RAG)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction à suivre pour exécution sur Google Colab\n",
    "\n",
    "Aller dans `Execution -> Modifier le type d'exécution` puis sélectionner `T4-GPU` pour exploiter les fonctionnalités GPU.\n",
    "\n",
    "![Colab GPU](resources/colab_gpu.png \"T4-GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des bibliothèques logicielles et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Vérifie si le code est exécuté sur Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Commandes à exécuter uniquement sur Google Colab\n",
    "    !git clone https://github.com/vincentmartin/tp-information-retrieval-with-llm-student-version.git\n",
    "    %cd tp-information-retrieval-with-llm-student-version\n",
    "    !pip install -r requirements.txt\n",
    "else:\n",
    "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
    "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données\n",
    "\n",
    "Ce premier bloc permet de lire les données. Grâce à [langchain](https://python.langchain.com/docs/get_started/introduction), lire des données est très facile et cela ne requiert que quelques lignes de codes.\n",
    "\n",
    "Etudier la document [Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) pour en apprendre plus sur la lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data_bbc_news/'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "print('{} documents lus.'.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Découpage des documents\n",
    "\n",
    "Les modèles _sentence transformers_ ne prennent qu'un nombre limité de tokens en entrée. C'est pourquoi il est nécessaire de découper les documents en plusieurs blocs. Ce découpage présente aussi l'avantage d'être plus précis dans la recherche et de ne cibler que les paragraphes les plus pertinents pour une recherche.\n",
    "\n",
    "Le bloc ci-dessous réalise ce découpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(documents,chunk_size=10000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "paragraphs = split_docs(documents)\n",
    "print('{} documents découpés en {} blocs.'.format(len(documents), len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du modèle _sentence transformer_\n",
    "\n",
    "La ligne ci-dessous permet de charger un modèle _sentence transformer_ permettant d'encoder un bloc de texte en un _embedding_, c'est à dire un vecteur de réels de taille fixe. Le modèle est automatiquement téléchargé sur la plateforme [Huggingface](https://huggingface.co).\n",
    "\n",
    "Jetez un oeil à la documentation des _sentence transformers_ ici : https://www.sbert.net/\n",
    "\n",
    "Certains modèle comme [celui-ci](https://huggingface.co/intfloat/multilingual-e5-base) sont multi langues.\n",
    "\n",
    "Vous pouvez remplacer le modèle par défaut par un autre modèle. Une liste de modèle est disponible ici https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "\n",
    "Dans la liste des modèle disponible ici https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads : \n",
    "- Rechercher le modèle `intfloat/multilingual-e5-base`.\n",
    "- Enlisant la documentation sur la page, spécifier la valeur du paramètre `model_name` avec le modèle trouvé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"A REMPLIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexation des documents dans la BDD vectorielle _ChromaDB_\n",
    "\n",
    "[ChromaDB](https://www.trychroma.com/) est une base de données vectorielle, un _Vector Store_. Elle permet de stocker des _embeddings_ pour des textes mais aussi pour des images et des fichiers audios.\n",
    "\n",
    "La ligne ci-dessous permet d'indexer les paragraphes calculés ci-dessus dans la base ChromaDB en utilisant le modèle _sentence transformer_ préalablement chargé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb_dir = \"./chromadb\" # Chemin où seront stockées les données\n",
    "db = Chroma.from_documents(paragraphs, embeddings, persist_directory=chromadb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrogation\n",
    "\n",
    "Interroger les documents est une étape simple pour l'utilisateur, même si les mécanismes sous-jacents restent complexes.\n",
    "\n",
    "La ligne ci-dessous permet de rechercher les documents similaire à la requête `query`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "\n",
    "En lisant la documentation ici https://python.langchain.com/docs/integrations/vectorstores/chroma : \n",
    "- Rechercher les documents similaires à la requête `query`\n",
    "- Afficher les 5 documents les plus pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Jeux vidéo Playstation\"\n",
    "matching_docs = ... # A REMPLIR\n",
    "\n",
    "# Affice les 5 résultats les plus pertinents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot en mode RAG\n",
    "\n",
    "Dans cette partie, nous mettons en oeuvre un chatbot en utilisant un _Large Language Model_ qui va se servir des documents trouvés dans la BDD vectorielle pour synthétiser et les informations et construire une réponse.\n",
    "\n",
    "Nous pourrions utiliser GPT3.5 ou GPT4 mais pour des raisons de coût (il faut un abonnement payant), nous allons utiliser un petit modèle open source [llama2](https://ai.meta.com/llama/).\n",
    "\n",
    "Nous allons même réaliser une petite IHM du chatbot avec la bibliothèque [gradio](https://www.gradio.app/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import gradio as gr\n",
    "\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration du LLM\n",
    "\n",
    "Dans cette partie, nous allons utiliser le modèle [llamav2](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) dôté de 7 milliards de paramètres. C'est le plus petit des trois modèles de l'organisation puisqu'il y a ensuite les modèles de 13 et 70 milliards de paramètres.\n",
    "\n",
    "Le modèle est automatiquement téléchargé sur la plateforme [Huggingface](https://huggingface.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Chargement de la configuration\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    llm_model_id\n",
    ")\n",
    "\n",
    "# Chargement du modèle LLM\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "\n",
    "# Configuration de la génération\n",
    "generation_config = GenerationConfig.from_pretrained(llm_model_id)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001 # plus la température est basse, plus les prédictions sont précises\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "# Création du pipeline\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Création du pipeline LLM\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du _prompt template_\n",
    "\n",
    "Un prompt template permet de configurer la manière dont le LLM doit se comporter. Grâce à la bibliothèque [langchain](https://www.langchain.com/), la création d'un _prompt template_ est simplifiée et permet notamment d'inclure des variables quis seront spécifiées plus tard, lors de l'exécution de la chaîne.\n",
    "\n",
    "Dans l'exemple ci-dessous, le template est rédigé en anglais car la majeure partie des documents sur lesquels le LLM a été entraîné sont en anglais. La performance est meilleur qu'en français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3\n",
    "\n",
    "Modifier le `prompt_template` pour indiquer dans le bloc `[INST]` les instructions nécessaire pour que le LLM : \n",
    "- Agisse comme un journaliste\n",
    "- Répondre à la question en utilisant seulement les éléments du contexte et rien d'autres\n",
    "- Cite les paragraphes pertinents qui ont permis de répondre à la questions\n",
    "\n",
    "Ces instructions doivent être écrites en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "[INST]\n",
    ">>A REMPLIR<<\n",
    "[/INST]\n",
    "\n",
    "[CONTEXT]\n",
    "{context}\n",
    "[/CONTEXT]\n",
    "\n",
    "[QUESTION]\n",
    "{question}\n",
    "[/QUESTION]\n",
    "\n",
    "[CITATION]\n",
    "Please cite specific passages from the above context that were particularly relevant in answering the question.\n",
    "[/CITATION]\n",
    "\n",
    "[RESPONSE]\n",
    "[/RESPONSE]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration de la chaîne de Question/Réponse\n",
    "\n",
    "Les lignes ci-dessous permettent de configurer le système de Question Réponse. Dans cet exemple, le système récupère les 2 paragraphes les plus pertinents (`\"k\":2`) pour construire la réponse. Il est possible d'augmenter ce nombre mais cela demandera plus de temps pour construire la réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, # LLM utilisé, en l'occrurence llamav2 7B\n",
    "    chain_type=\"stuff\", # Type de chaîne à utiliser\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}), # récupérateur des k documents les plus pertinents\n",
    "    return_source_documents=True, # retourne les documents sources\n",
    "    chain_type_kwargs={\"prompt\": prompt}, # prompt à utiliser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IHM du chatbot avec gradio\n",
    "\n",
    "Les quelques lignes ci-dessous permettent de construire l'IHM du chatbot qui sera accessible depuis un navigateur Web. Depuis google colab, un lien générique durant 24H est généré automatiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, history):\n",
    "    response = qa_chain(\n",
    "      question\n",
    "    )\n",
    "    return response[\"result\"].strip()\n",
    "\n",
    "interface = gr.ChatInterface(\n",
    "    fn = ask,\n",
    "    chatbot=gr.Chatbot(height=600),\n",
    "    textbox=gr.Textbox(placeholder=\"What is quantum computing?\", container=False, scale=7),\n",
    "    title=\"CNAM ChatBot\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What is quantum computing?\"],\n",
    "\n",
    "    cache_examples=True,\n",
    "    retry_btn=\"Relancer\",\n",
    "    undo_btn=\"Annuler\",\n",
    "    clear_btn=\"Réinitialiser\",\n",
    "    submit_btn=\"Envoyer\"\n",
    "\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4. \n",
    "\n",
    "En utilisant le chatbot construit, indiquer ci-dessous les réponses aux questions ci-après."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quelques exemples de questions à poser\n",
    "\n",
    "- Who is Dr Mario Paniccia?\n",
    "- What celebrity opens February's Super Bowl?\n",
    "- What is the primary objective of the upcoming meeting of military chiefs regarding Scotland's Army regiments?\n",
    "- What injury did Chris Tomlinson sustain, and how has it affected his training and competition plans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 5.\n",
    "\n",
    "Modifier le `prompt_template` pour que le chatbot réponde en français."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
